{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":36,"outputs":[{"output_type":"stream","text":"/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\n/kaggle/input/datafornewssummarisation/news_summary.csv\n/kaggle/input/datafornewssummarisation/summary.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import model_from_json\nimport numpy as np\nfrom nltk import word_tokenize\nfrom numpy import array\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import LSTM\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing import sequence\nfrom keras.layers import Dense\nfrom keras.preprocessing.sequence import pad_sequences\nimport re,codecs\nfrom nltk.corpus import stopwords\nfrom bs4 import BeautifulSoup \nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\n##You may import any other module if required","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n\n                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n\n                           \"you're\": \"you are\", \"you've\": \"you have\"}\n\n","execution_count":34,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"df=pd.read_csv(\"../input/datafornewssummarisation/news_summary.csv\",encoding = \"ISO-8859-1\")\ncolumns=df.columns\nnew_df=df[['ctext','text']]\n#print(df.describe())\n#print(new_df.head())\n#print(\"Description:\",new_df['ctext'][0])\n#print(\"Summary:\",new_df['text'][0])\nraw=pd.read_csv(\"../input/datafornewssummarisation/summary.csv\",encoding = \"ISO-8859-1\")\nraw.to_csv(r'../working/datafornewssummarisation\\summary.csv')\n#new_df.to_csv(raw,encoding = \"ISO-8859-1\")","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.to_csv(\"../working/datafornewssummarisation\\summary.csv\")","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../working/datafornewssummarisation\\summary.csv\",encoding = \"ISO-8859-1\")\ncolumns=df.columns\n\n\n\n\n#An example showing the data and the summary\n#print(\"Text:\",X[1])\n#print(\"\\n\")\n#print(\"Summary:\",y[1])\n#Text cleaning\n#Rows having duplicates will be omitted\ndf.drop_duplicates(keep='first',inplace=True)\n\n#Rows not having summary or document is deleted\ndf.dropna(axis=0,inplace=True)\nX=df['ctext']\ny=df['text']","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the dataset to train set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dataset_cleaning():\n    stop_words = set(stopwords.words('english')) \n    def text_cleaner(text):\n        newString = text.lower()\n        newString = BeautifulSoup(newString, \"lxml\").text\n        newString = re.sub(r'\\([^)]*\\)', '', newString)\n        newString = re.sub('\"','', newString)\n        newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n        newString = re.sub(r\"'s\\b\",\"\",newString)\n        newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n        tokens = [w for w in newString.split() if not w in stop_words]\n        long_words=[]\n        for i in tokens:\n            if len(i)>=3:                  #removing short word\n                long_words.append(i)   \n        return (\" \".join(long_words)).strip()\n\n    cleaned_text = []\n    for t in df['ctext']:\n        cleaned_text.append(text_cleaner(t))\n    print('done')\n\n    def summary_cleaner(text):\n        newString = re.sub('\"','', text)\n        newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n        newString = re.sub(r\"'s\\b\",\"\",newString)\n        newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n        newString = newString.lower()\n        tokens=newString.split()\n        newString=''\n        for i in tokens:\n            if len(i)>1:                                 \n                newString=newString+i+' '  \n        return newString\n\n    #Call the above function\n    cleaned_summary = []\n    for t in df['text']:\n        cleaned_summary.append(summary_cleaner(t))\n    \n\n    df['cleaned_text']=cleaned_text\n    df['cleaned_summary']=cleaned_summary\n    df['cleaned_summary'].replace('', np.nan, inplace=True)\n    df.dropna(axis=0,inplace=True)","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_cleaning()","execution_count":42,"outputs":[{"output_type":"stream","text":"done\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(X_train,X_test,y_train,y_test):\n    #This function  will be used to preprocess the data to a form that can be fed into the neural network\n    #Subparts:\n    ##1.Find the most_probable_length of the sentences of text_type and summary type \n    ##(Hint: you may plot the distribution of lengths of sentences)\n    text_count = []\n    summary_count = []\n\n    for t in df['cleaned_text']:\n        text_count.append(len(t.split()))\n    for t in df['cleaned_summary']:\n        summary_count.append(len(t.split()))\n      \n   #graph_df= pd.DataFrame()\n   #graph_df['text']=text_count\n    df2=pd.DataFrame()\n  # graph_df['summary']=summary_count\n    df2['summary']=summary_count\n  # graph_df.hist(bins = 5,range = [0,600])\n  # df2.hist(bins = 5,range = [50,63])\n   #plt.show()\n    max_sum_len=63\n    max_text_len=600\n    \n\n    ##2.Tokenizing the data\n    X_tokenizer = Tokenizer()\n    X_tokenizer.fit_on_texts(list(X_train))\n    y_tokenizer = Tokenizer()\n    y_tokenizer.fit_on_texts(list(y_train))\n    \n    X_train_seq = X_tokenizer.texts_to_sequences(X_train)\n    X_test_seq=X_tokenizer.texts_to_sequences(X_test)\n    y_train_seq = y_tokenizer.texts_to_sequences(y_train)\n    y_test_seq=y_tokenizer.texts_to_sequences(y_test)\n    \n    ##3.Padding the sentences to a specific length\n    X_train=pad_sequences(X_train_seq,maxlen=max_text_len)\n    X_test= pad_sequences(X_test_seq,maxlen=max_text_len)\n    y_train=pad_sequences(y_train_seq,maxlen=max_sum_len)\n    y_test= pad_sequences(y_test_seq,maxlen=max_sum_len)  \n\n    ##4.Introducing embeddings( We'll be using the Fasttext embeddings)\n    embeddings_index = {}\n    f = open('../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec', encoding='utf-8')\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n\n    print('Found %s word vectors.' % len(embeddings_index))\n    word_index = X_tokenizer.word_index\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessing(X_train,X_test,y_train,y_test)","execution_count":46,"outputs":[{"output_type":"stream","text":"Found 2000000 word vectors.\n","name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"could not broadcast input array from shape (300) into shape (15)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-66a7afe5e444>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-45-61e859ec963a>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# words not found in embedding index will be all-zeros.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (300) into shape (15)"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}