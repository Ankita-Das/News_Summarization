{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":38,"outputs":[{"output_type":"stream","text":"/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\n/kaggle/input/news-summary/news_summary.csv\n/kaggle/input/news-summary/news_summary_more.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import model_from_json\nimport numpy as np\nfrom nltk import word_tokenize\nfrom numpy import array\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Input\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing import sequence\nfrom keras.layers import Dense\nfrom keras.preprocessing.sequence import pad_sequences\nimport re,codecs\nfrom nltk.corpus import stopwords\nfrom bs4 import BeautifulSoup \nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n\n                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n\n                           \"you're\": \"you are\", \"you've\": \"you have\"}","execution_count":40,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"../input/news-summary/news_summary.csv\",encoding = \"ISO-8859-1\")\n#news_summary_more = pd.read_csv(\"../input/news-summary/news_summary_more.csv\",encoding = \"ISO-8859-1\")\ncolumns=df.columns\nnew_df=df[['ctext','text']]\n#print(df.describe())\n#print(new_df.head())\n#print(\"Description:\",new_df['ctext'][0])\n#print(\"Summary:\",new_df['text'][0])\n#raw=pd.read_csv(\"../input/datafornewssummarisation/summary.csv\",encoding = \"ISO-8859-1\")\n#raw.to_csv(r'../working/datafornewssummarisation\\summary.csv')\nnew_df.to_csv('summary.csv',encoding = \"ISO-8859-1\")\n","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../working/summary.csv\",encoding = \"ISO-8859-1\")\ncolumns=df.columns\n\nprint(columns)\n\n\n#An example showing the data and the summary\n#print(\"Text:\",X[1])\n#print(\"\\n\")\n#print(\"Summary:\",y[1])\n#Text cleaning\n#Rows having duplicates will be omitted\ndf.drop_duplicates(keep='first',inplace=True)\n\n#Rows not having summary or document is deleted\ndf.dropna(axis=0,inplace=True)\nX=df['ctext']\ny=df['text']","execution_count":42,"outputs":[{"output_type":"stream","text":"Index(['Unnamed: 0', 'ctext', 'text'], dtype='object')\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#Splitting the dataset to train set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n\n","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dataset_cleaning():\n    stop_words = set(stopwords.words('english')) \n    def text_cleaner(text):\n        newString = text.lower()\n        newString = BeautifulSoup(newString, \"lxml\").text\n        newString = re.sub(r'\\([^)]*\\)', '', newString)\n        newString = re.sub('\"','', newString)\n        newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n        newString = re.sub(r\"'s\\b\",\"\",newString)\n        newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n        tokens = [w for w in newString.split() if not w in stop_words]\n        long_words=[]\n        for i in tokens:\n            if len(i)>=3:                  #removing short word\n                long_words.append(i)   \n        return (\" \".join(long_words)).strip()\n\n    cleaned_text = []\n    for t in df['ctext']:\n        cleaned_text.append(text_cleaner(t))\n    print('done')\n\n    def summary_cleaner(text):\n        newString = re.sub('\"','', text)\n        newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n        newString = re.sub(r\"'s\\b\",\"\",newString)\n        newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n        newString = newString.lower()\n        tokens=newString.split()\n        newString=''\n        for i in tokens:\n            if len(i)>1:                                 \n                newString=newString+i+' '  \n        return newString\n\n    #Call the above function\n    cleaned_summary = []\n    for t in df['text']:\n        cleaned_summary.append(summary_cleaner(t))\n    \n\n    df['cleaned_text']=cleaned_text\n    df['cleaned_summary']=cleaned_summary\n    df['cleaned_summary'].replace('', np.nan, inplace=True)\n    df.dropna(axis=0,inplace=True)\n\n\n","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_cleaning()\n","execution_count":45,"outputs":[{"output_type":"stream","text":"done\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(X_train,X_test,y_train,y_test):\n    #This function  will be used to preprocess the data to a form that can be fed into the neural network\n    #Subparts:\n    ##1.Find the most_probable_length of the sentences of text_type and summary type \n    ##(Hint: you may plot the distribution of lengths of sentences)\n    text_count = []\n    summary_count = []\n\n    for t in df['cleaned_text']:\n        text_count.append(len(t.split()))\n    for t in df['cleaned_summary']:\n        summary_count.append(len(t.split()))\n      \n   #graph_df= pd.DataFrame()\n   #graph_df['text']=text_count\n    df2=pd.DataFrame()\n  # graph_df['summary']=summary_count\n    df2['summary']=summary_count\n  # graph_df.hist(bins = 5,range = [0,600])\n  # df2.hist(bins = 5,range = [50,63])\n   #plt.show()\n    max_sum_len=63\n    max_text_len=600\n    \n\n    ##2.Tokenizing the data\n    X_tokenizer = Tokenizer()\n    X_tokenizer.fit_on_texts(list(X_train))\n    total_X=0\n    for key,value in X_tokenizer.word_counts.items():\n        total_X=total_X+1\n    X_tokenizer=Tokenizer(num_words=total_X)\n    X_tokenizer.fit_on_texts(list(X_train))\n    \n    y_tokenizer = Tokenizer()\n    y_tokenizer.fit_on_texts(list(y_train))\n    total_y=0\n    for key,value in y_tokenizer.word_counts.items():\n        total_y=total_y+1\n    y_tokenizer=Tokenizer(num_words=total_y)\n    y_tokenizer.fit_on_texts(list(y_train))\n    \n    X_train_seq = X_tokenizer.texts_to_sequences(X_train)\n    X_test_seq=X_tokenizer.texts_to_sequences(X_test)\n    y_train_seq = y_tokenizer.texts_to_sequences(y_train)\n    y_test_seq=y_tokenizer.texts_to_sequences(y_test)\n    \n    ##3.Padding the sentences to a specific length\n    X_train=pad_sequences(X_train_seq,maxlen=max_text_len)\n    X_test= pad_sequences(X_test_seq,maxlen=max_text_len)\n    y_train=pad_sequences(y_train_seq,maxlen=max_sum_len)\n    y_test= pad_sequences(y_test_seq,maxlen=max_sum_len)  \n    X_voc=X_tokenizer.num_words +1\n    y_voc=y_tokenizer.num_words +1\n\n    ##4.Introducing embeddings( We'll be using the Fasttext embeddings)\n    embeddings_index = {}\n    f = open('../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec', encoding='utf-8')\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n\n    word_index = X_tokenizer.word_index\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    print(\"done\")\n    return embedding_matrix,X_voc,y_voc","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix,X_voc,y_voc=preprocessing(X_train,X_test,y_train,y_test)","execution_count":47,"outputs":[{"output_type":"stream","text":"done\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(embedding_matrix,X_voc,y_voc,max_text_len):\n    from keras import backend as k\n    k.clear_session()\n    \n    encoder_input=Input(shape=(1,max_text_len))\n    encoder_emb=Embedding(X_voc,300,weights=[embedding_matrix],trainable=True)(encoder_input)\n    encoder_lstm=LSTM(300,return_sequences=True,return_state=True)\n    encoder_output, state_h, state_c= encoder_lstm(encoder_input)\n    \n    decoder_input=Input(shape=(None,))\n    decoder_emb=Embedding(y_voc,300,trainable=True)(decoder_input)\n    decoder_lstm=LSTM(300,return_sequences=True,return_state=True)   \n    decoder_output,decoder_forward_state, decoder_back_state= decoder_lstm(decoder_emb,initial_state=[state_h, state_c])\n    from keras.layers import TimeDistributed,Dense\n    decoder_dense=TimeDistributed(Dense(y_voc,activation='softmax'))\n    decoder_output=decoder_dense(decoder_output)\n    \n    model=Model([encoder_input,decoder_input],decoder_output)\n    model.summary()","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_text_len=600\n#print(embedding_matrix.shape)\n#print(X_voc)\nmodel(embedding_matrix,X_voc,y_voc,max_text_len)","execution_count":49,"outputs":[{"output_type":"stream","text":"Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            (None, None)         0                                            \n__________________________________________________________________________________________________\ninput_1 (InputLayer)            (None, 1, 600)       0                                            \n__________________________________________________________________________________________________\nembedding_2 (Embedding)         (None, None, 300)    5166900     input_2[0][0]                    \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   [(None, 1, 300), (No 1081200     input_1[0][0]                    \n__________________________________________________________________________________________________\nlstm_2 (LSTM)                   [(None, None, 300),  721200      embedding_2[0][0]                \n                                                                 lstm_1[0][1]                     \n                                                                 lstm_1[0][2]                     \n__________________________________________________________________________________________________\ntime_distributed_1 (TimeDistrib (None, None, 17223)  5184123     lstm_2[0][0]                     \n==================================================================================================\nTotal params: 12,153,423\nTrainable params: 12,153,423\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(embedding_matrix.shape)","execution_count":52,"outputs":[{"output_type":"stream","text":"(45476, 300)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Model is built and summary shown above"},{"metadata":{"trusted":true},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}